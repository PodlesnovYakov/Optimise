\doxysection{Det\+Opt Class Reference}
\hypertarget{class_det_opt}{}\label{class_det_opt}\index{DetOpt@{DetOpt}}


A descendant class of the \doxylink{class_abstr_optimiser}{Abstr\+Optimiser} class, which performs the role of finding the minimum of a function using gradient descent.  




{\ttfamily \#include $<$Det\+Opt.\+h$>$}

Inheritance diagram for Det\+Opt\+:\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[height=2.000000cm]{class_det_opt}
\end{center}
\end{figure}
\doxysubsubsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
void \mbox{\hyperlink{class_det_opt_aa74f1b8bf1b7bf94b1f5e2cf645e4710}{optimise}} (vec \&point, \mbox{\hyperlink{class_abstr_func}{Abstr\+Func}} \texorpdfstring{$\ast$}{*}Func, \mbox{\hyperlink{class_abstr_stop}{Abstr\+Stop}} \texorpdfstring{$\ast$}{*}S, \mbox{\hyperlink{class_rect_area}{Rect\+Area}} \&Shape)
\begin{DoxyCompactList}\small\item\em This method is designed to find the minimum of an n-\/dimensional function. \end{DoxyCompactList}\item 
vec \mbox{\hyperlink{class_det_opt_a7e05250a2321531db326061a23a8f916}{Calc\+Gradient}} (vec \&point, \mbox{\hyperlink{class_abstr_func}{Abstr\+Func}} \&Func, \mbox{\hyperlink{class_rect_area}{Rect\+Area}} \&Shape)
\begin{DoxyCompactList}\small\item\em This method calculates numerically the gradient of a function at a point. \end{DoxyCompactList}\item 
double \mbox{\hyperlink{class_det_opt_a23920cdcf918151d374775723062b3d8}{Golden\+Selection}} (double a, double b, vec \&gradient, vec \&point, \mbox{\hyperlink{class_abstr_func}{Abstr\+Func}} \texorpdfstring{$\ast$}{*}Func)
\begin{DoxyCompactList}\small\item\em This method (golden ratio) considers the next step in the direction of gradient descent. \end{DoxyCompactList}\item 
double \mbox{\hyperlink{class_det_opt_a102b8132371f4c0504fc1973bfe912f8}{CountB}} (vec \&point, vec \&gradient, \mbox{\hyperlink{class_rect_area}{Rect\+Area}} \&Shape)
\begin{DoxyCompactList}\small\item\em This method finds the end of the interval in which we are looking for the next step in the gradient descent direction. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
A descendant class of the \doxylink{class_abstr_optimiser}{Abstr\+Optimiser} class, which performs the role of finding the minimum of a function using gradient descent. 

\doxysubsection{Member Function Documentation}
\Hypertarget{class_det_opt_a7e05250a2321531db326061a23a8f916}\label{class_det_opt_a7e05250a2321531db326061a23a8f916} 
\index{DetOpt@{DetOpt}!CalcGradient@{CalcGradient}}
\index{CalcGradient@{CalcGradient}!DetOpt@{DetOpt}}
\doxysubsubsection{\texorpdfstring{CalcGradient()}{CalcGradient()}}
{\footnotesize\ttfamily vec Det\+Opt\+::\+Calc\+Gradient (\begin{DoxyParamCaption}\item[{vec \&}]{point,  }\item[{\mbox{\hyperlink{class_abstr_func}{Abstr\+Func}} \&}]{Func,  }\item[{\mbox{\hyperlink{class_rect_area}{Rect\+Area}} \&}]{Shape }\end{DoxyParamCaption})}



This method calculates numerically the gradient of a function at a point. 


\begin{DoxyParams}{Parameters}
{\em point} & -\/ the point at which we calculate the gradient. \\
\hline
{\em Func} & -\/ function, the gradient at the point of which we are looking for. \\
\hline
{\em Shape} & -\/ parallelepiped. \\
\hline
\end{DoxyParams}
\Hypertarget{class_det_opt_a102b8132371f4c0504fc1973bfe912f8}\label{class_det_opt_a102b8132371f4c0504fc1973bfe912f8} 
\index{DetOpt@{DetOpt}!CountB@{CountB}}
\index{CountB@{CountB}!DetOpt@{DetOpt}}
\doxysubsubsection{\texorpdfstring{CountB()}{CountB()}}
{\footnotesize\ttfamily double Det\+Opt\+::\+CountB (\begin{DoxyParamCaption}\item[{vec \&}]{point,  }\item[{vec \&}]{gradient,  }\item[{\mbox{\hyperlink{class_rect_area}{Rect\+Area}} \&}]{Shape }\end{DoxyParamCaption})}



This method finds the end of the interval in which we are looking for the next step in the gradient descent direction. 


\begin{DoxyParams}{Parameters}
{\em point} & -\/ current direction point. \\
\hline
{\em gradient} & -\/ gradient calculated at a point. \\
\hline
{\em Shape} & -\/ parallelepiped. \\
\hline
\end{DoxyParams}
\Hypertarget{class_det_opt_a23920cdcf918151d374775723062b3d8}\label{class_det_opt_a23920cdcf918151d374775723062b3d8} 
\index{DetOpt@{DetOpt}!GoldenSelection@{GoldenSelection}}
\index{GoldenSelection@{GoldenSelection}!DetOpt@{DetOpt}}
\doxysubsubsection{\texorpdfstring{GoldenSelection()}{GoldenSelection()}}
{\footnotesize\ttfamily double Det\+Opt\+::\+Golden\+Selection (\begin{DoxyParamCaption}\item[{double}]{a,  }\item[{double}]{b,  }\item[{vec \&}]{gradient,  }\item[{vec \&}]{point,  }\item[{\mbox{\hyperlink{class_abstr_func}{Abstr\+Func}} \texorpdfstring{$\ast$}{*}}]{Func }\end{DoxyParamCaption})}



This method (golden ratio) considers the next step in the direction of gradient descent. 


\begin{DoxyParams}{Parameters}
{\em a} & is the beginning of the interval in which we are looking for the next step in the gradient descent direction. \\
\hline
{\em b} & -\/ the end of the interval in which we are looking for the next step in the direction of gradient descent. \\
\hline
{\em gradient} & -\/ gradient calculated at a point. \\
\hline
{\em point} & -\/ current direction point. \\
\hline
{\em Func} & is the function that we are minimizing. \\
\hline
\end{DoxyParams}
\Hypertarget{class_det_opt_aa74f1b8bf1b7bf94b1f5e2cf645e4710}\label{class_det_opt_aa74f1b8bf1b7bf94b1f5e2cf645e4710} 
\index{DetOpt@{DetOpt}!optimise@{optimise}}
\index{optimise@{optimise}!DetOpt@{DetOpt}}
\doxysubsubsection{\texorpdfstring{optimise()}{optimise()}}
{\footnotesize\ttfamily void Det\+Opt\+::optimise (\begin{DoxyParamCaption}\item[{vec \&}]{point,  }\item[{\mbox{\hyperlink{class_abstr_func}{Abstr\+Func}} \texorpdfstring{$\ast$}{*}}]{Func,  }\item[{\mbox{\hyperlink{class_abstr_stop}{Abstr\+Stop}} \texorpdfstring{$\ast$}{*}}]{S,  }\item[{\mbox{\hyperlink{class_rect_area}{Rect\+Area}} \&}]{Shape }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



This method is designed to find the minimum of an n-\/dimensional function. 


\begin{DoxyParams}{Parameters}
{\em point} & -\/ the starting point from which the search will begin. \\
\hline
{\em Func} & is the minimum function we are looking for. \\
\hline
{\em S} & is the condition that will cause the optimizer to stop. \\
\hline
{\em Shape} & is an n-\/dimensional parallelepiped. \\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{class_abstr_optimiser_ae4d677a4a7ad5caf8d9a8addf12e84b1}{Abstr\+Optimiser}}.



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
Det\+Opt.\+h\item 
Det\+Opt.\+cpp\end{DoxyCompactItemize}
